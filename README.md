# InsightPlaces | 2¬∫ Challenge Data Science Alura

A imobili√°ria InsightPlaces, situada na cidade do Rio de Janeiro, est√° enfrentando dificuldades para alugar e vender im√≥veis. Em uma pesquisa de como empresas semelhantes operam no mercado, a InsightPlaces percebeu que esse problema pode estar relacionado aos valores dos im√≥veis e √†s recomenda√ß√µes realizadas em seu site.

| :placard: Vitrine.Dev |                                                                      |
| --------------------- | -------------------------------------------------------------------- |
| ‚ú® Nome               | **InsightPlaces**                                              |
| üè∑ Tecnologias        | Python, PySpark                                                      |
| üöÄ URL                | https://github.com/JuanEnD/InsightPlaces                             |
| üî• Desafio            | [Link do Challenge](https://www.alura.com.br/challenges/data-science-2) |

<!-- Inserir imagem com a #vitrinedev ao final do link -->

![Empresa InsightsPlaces do projeto](https://i.imgur.com/Tnj84r9.jpg#vitrinedev)

## Detalhes do projeto

Esse projeto tem algumas etapas, como: ler e fazer o tratamento do hist√≥rico dos pre√ßos de im√≥veis no Rio de Janeiro, construir um modelo de regress√£o para precificar im√≥veis e criar um recomendado de im√≥veis. Usando os m√©todos do PySpark, que √© uma interface para Apache Spark em Python.

O projeto usou uma base de dados de im√≥veis na cidade do Rio de Janeiro, onde a InsightPlaces disponibilizou os dados contidos no banco de dados quanto √†s informa√ß√µes de pre√ßos de im√≥veis da cidade do Rio de Janeiro.

[Base de dados - InsightPlaces](https://caelum-online-public.s3.amazonaws.com/challenge-spark/semana-1.zip)

### Dicion√°rio de Dados - Anuncio

| Colunas         | Descri√ß√£o                                                        |
| --------------- | ------------------------------------------------------------------ |
| id              | C√≥digo de identifica√ß√£o do an√∫ncio no sistema da InsightPlaces |
| tipo_unidade    | Tipo de im√≥vel (apartamento, casa e outros)                       |
| tipo_uso        | Tipo de uso do im√≥vel (residencial ou comercial)                  |
| area_total      | √Årea total do im√≥vel (constru√ß√£o e terreno)                    |
| area_util       | √Årea constru√≠da do im√≥vel                                       |
| quartos         | Quantidade de quartos do im√≥vel                                   |
| suites          | Quantidade de su√≠tes do im√≥vel                                   |
| banheiros       | Quantidade de banheiros do im√≥vel                                 |
| vaga            | Quantidade de vagas de garagem do im√≥vel                          |
| caracteristicas | Listagem de caracter√≠sticas do im√≥vel                            |
| andar           | N√∫mero do andar do im√≥vel                                        |
| endereco        | Informa√ß√µes sobre o endere√ßo do im√≥vel                         |
| valores         | Informa√ß√µes sobre valores de venda e loca√ß√£o dos im√≥veis      |

## Sobre o desafio

### Etapas do projeto

- **‚úîÔ∏è Semana 1:** Transforma√ß√£o dos dados com PySpark: Explorar e tratar uma base de dados que veio dos sistemas internos da empresa InsightPlaces.
- **‚úîÔ∏è Semana 2:** Tratamento dos dados e cria√ß√£o de um modelo de regress√£o com PySpark: Criar um modelo de regress√£o para prever os valores dos im√≥veis.
- **‚úîÔ∏è Semana 3 e 4:** Criando um modelo de recomenda√ß√£o com PySpark: Onde vou criar um recomendador de im√≥veis.

<!-- -   **Semana 4:** Ajustes gerais e corre√ß√µes de bugs. -->

---

### -  **1¬∫ semana:** 

Na primeira semana eu baixei a biblioteca do PySpark para iniciar a leitura da base de dados para utilizar no projeto, depois de ler os dados fui explorar para tratar os dados, separar cada um dos camps da Base de Dados e deixar apenas as informa√ß√µes do campo "anuncio" j√° que nessa base tinha 3 campos principais que eram: Anuncio, Imagem e Usuario.

Ent√£o, como precisaria para analisar apenas as informa√ß√µes do campo Anuncio eu apaguei as outras e foquei em analisar as colunas desse campo. Ap√≥s fazer essa separa√ß√£o comecei a analisar e fazer todos os tratamentos de cada uma das colunas, um deles foi transformar os dados das colunas "quartos", "suites", "banheiros", "vaga", "area_total" e "area_util" de listas para inteiros. Em seguida, precisei extrair da coluna "endereco" apenas as informa√ß√µes de "zona" e "bairro" para transformar apenas elas duas como colunas no DataFrame, logo depois fui separar os campos da coluna "valores" para cada uma ser um campo ser coluna assim como tinha feito na coluna "endereco" j√° que nessa coluna "valores" tinha as informa√ß√µes de condominio, iptu, tipo, e valor.

Por √∫ltimo ap√≥s tratar todos esses dados finalizei o Notebook salvando esse DataFrame que tinha criado com todas as informa√ß√µes de Anuncio no formato Parquet, que √© um formato de arquivo de c√≥digo aberto e orientado por colunas, ele √© bastante eficiente quando se trata de armazenar e analisar grandes volumes de dados. Tamb√©m salvei o DataFrame em CSV, depois fiz um teste de desempenho entre a leitura dos dois arquivos.

---

### -  **2¬∫ semana:** 

No inicio do Notebook dessa semana utilizei a base de dados que teve algumas transforma√ß√µes e salvei no final da primeira semana, depois fiz uma etapa muito importante na constru√ß√£o de modelos de Machine Learning que √© a sele√ß√£o de features, escolhendo quais colunas s√£o importante e descartar as irrelevantes para a an√°lise reduzindo a complexidade dos dados inseridos que por conseequ√™ncia o modelo ir√° demorar menos tempo para ser ajustado. Nesse processo descartei as colunas ("area_total", "tipo_anuncio", "tipo_unidade", "tipo_uso" e "tipo") e antes existia duas colunas com muitos dados iguais que eram a ('area_util', 'area_total'), ent√£o verifiquei qual das duas tinha mais dados nulos e descobri que era a ("area_total").

Em seguida, converti os tipos de algumas colunas que estavam erradas, como por exemplo de num√©ricas das colunas "andar", "banheiros", "suites" e "quartos" para o tipo inteiro. Al√©m disso tamb√©m converti as colunas "area_util", "condominio", "iptu" e "valor" para o tipo double. Ap√≥s isso fui fazer um tratamento da coluna caracter√≠sticas, porque ela possuia listas de strings como conte√∫do de suas linhas, mas algumas dessas listas est√£o sem elementos. Ent√£o transformei essas listas sem elementos em valores nulos utilizando os recursos da biblioteca **functions** do PySpark, depois fiz um tratamento com os dados faltantes (nulos e NaN's) porque possui muito deles em todo o DataFrame e os modelos de Machine Learning n√£o costumam trabalhar corretamente com dados nulos.

Ent√£o ap√≥s todos esses tratamentos nas colunas comecei a prepara√ß√£o dos dados para os algoritmos do **Spark MLlib**, utilizei a t√©cnica de vetoriza√ß√£o dos dados e usei a classe **VectorAssembler** da biblioteca **pyspark.ml.feature**. Em seguida comecei a constru√ß√£o dos modelos de regress√£o para auxiliar na previs√£o dos valores de im√≥veis ap√≥s os dados j√° estarem vetorizados, o primeiro modelo utilizado foi o **Random Forest** que √© um modelo de Machine Learning que utiliza a t√©cnica de ensemble learnin (aprendizado de ensemble), ele √© composto por uma cole√ß√£o de √°rvores de decis√£o onde cada uma dessas √°rvores √© treinada com uma amostra aleat√≥ria do conjunto de dados. Assim o modelo no final faz a m√©dia dos resultados de cada uma das √°rvores para obter a predi√ß√£o mais correta.

**[Todos os Detalhes](https://github.com/JuanEnD/InsightPlaces/tree/main/Semana_2)**

---

### -  **3¬∫ e 4¬∫ semana:**

Iniciei o notebook utilizando a base de dados que tinha feito o tratamento na √∫ltima semana para utilizar no modelo de Machine Learning de Regress√£o. Ap√≥s isso, apliquei as transforma√ß√µes para utilizar os dados no modelo de clustering utilizando o **VectorAssembler** e fiz uma padroniza√ß√£o nos dados para consegui utilizar o PCA neles, por meio da classe **StandardScaler** **do PySpark** para realizar esse processo e criar **o** modelo de recomenda√ß√£o, mas antes disso preciso reduzir a dimens√£o dos dados e para fazer isso preciso utilizar a t√©cnica chamada PCA assim teria um novo conjunto de dados onde as colunas ser√£o uma combina√ß√£o linear das colunas originais. Ap√≥s a transforma√ß√£o com o PCA posso utilizar o m√©todo **explainedVariance**que retorna a vari√¢ncia explicada por cada componente principal, com esse m√©todo conseguiria encontrar o n√∫mero ideal de componentes princiapis que os dados devem possuir.

Em seguida, construi um Pipeline para facilitar a aplica√ß√£o das transforma√ß√µes nos dados e o que antes eram v√°rios processos separados agora era apenas um que fazia tudo desde o VectorAssembler at√© o PCA. Ent√£o depois de terminar esses tratamentos comecei a criar os Clusters com Kmeans que √© um algoritmo de aprendizado n√£o supervisionado que tem como objetivo encontrar grupos de dados que possuem caracter√≠sticas semelhantes e colocar todo em um mesmo Cluster. Tamb√©m avaliei os dados de cada um dos Clusters para descobrir as informa√ß√µes de todos os im√≥veis, como quantidade de banheiros, quartos e tamanho do im√≥vel. Ap√≥s isso fiz uma representa√ß√£o por meio de um gr√°fico de todos os dados dos im√≥veis que est√£o no Cluster, onde poderia ver qual o Cluster especifico e quais bairros s√£o de cada um dos dados.

Ent√£o ao fazer todos esses tratamentos e criar os Clusters pude filtrar im√≥veis de um mesmo Cluster com um ID, al√©m disso calculei a dist√¢ncia euclid√≠ana para encontrar as 10 melhores recomenda√ß√µes. No final criei uma fun√ß√£o recomendadora para que os Dev da **InsightPlaces** conseguissem utilizar o recomendador no site, por isso nessa fun√ß√£o coloquei todas as outras etapas que tinha feito antes dos im√≥veis, recomendados e procurado.
