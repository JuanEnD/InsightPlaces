# - **2º semana:**

No inicio do Notebook dessa semana utilizei a base de dados que teve algumas transformações e salvei no final da primeira semana, depois fiz uma etapa muito importante na construção de modelos de Machine Learning que é a seleção de features, escolhendo quais colunas são importante e descartar as irrelevantes para a análise reduzindo a complexidade dos dados inseridos que por conseequência o modelo irá demorar menos tempo para ser ajustado. Nesse processo descartei as colunas ("area_total", "tipo_anuncio", "tipo_unidade", "tipo_uso" e "tipo") e antes existia duas colunas com muitos dados iguais que eram a ('area_util', 'area_total'), então verifiquei qual das duas tinha mais dados nulos e descobri que era a ("area_total").

Em seguida, converti os tipos de algumas colunas que estavam erradas, como por exemplo de numéricas das colunas "andar", "banheiros", "suites" e "quartos" para o tipo inteiro. Além disso também converti as colunas "area_util", "condominio", "iptu" e "valor" para o tipo double. Após isso fui fazer um tratamento da coluna características, porque ela possuia listas de strings como conteúdo de suas linhas, mas algumas dessas listas estão sem elementos. Então transformei essas listas sem elementos em valores nulos utilizando os recursos da biblioteca functions do PySpark, depois fiz um tratamento com os dados faltantes (nulos e NaN's) porque possui muito deles em todo o DataFrame e os modelos de Machine Learning não costumam trabalhar corretamente com dados nulos.

Logo depois fiz uma preparação dos dados para Machine Learning e conseguir utilizar em modelos de Machine Learning utilizei algumas técnicas, como a transformação de variáveis categóricas em binárias por meio do processo chamado Variáveis Dummy que consiste em transformar as variáveis categóricas em binárias nas colunas caracteristicas e zona, precisa fazer isso pois modelos de regressão não conseguem lidar com variáveis textuais. depois salvei essa base de dados tratada em um novo arquivo Parquet para utilizar em modelos de Machine Learning.

Então após todos esses tratamentos nas colunas comecei a preparação dos dados para os algoritmos do Spark MLlib, utilizei a técnica de vetorização dos dados e usei a classe VectorAssembler da biblioteca pyspark.ml.feature. Em seguida comecei a construção dos modelos de regressão para auxiliar na previsão dos valores de imóveis após os dados já estarem vetorizados, o primeiro modelo utilizado foi o Random Forest que é um modelo de Machine Learning que utiliza a técnica de ensemble learnin (aprendizado de ensemble), ele é composto por uma coleção de árvores de decisão onde cada uma dessas árvores é treinada com uma amostra aleatória do conjunto de dados. Assim o modelo no final faz a média dos resultados de cada uma das árvores para obter a predição mais correta.

No final ao terminar os modelos fiz uma avaliação dos modelos utilizando as métricas:

RMSE (ou Raiz do Erro Quadrático Médio): é a métrica que calcula a raiz quadrada do erro médio entre os valores reais e as predições da regressão. Quanto menor esse valor, melhor será o modelo, uma vez que estará cometendo menos erros.

R2 (ou coeficiente de determinação): é uma medida estatística que representa a porcentagem de variação da variável resposta que é explicada pelo modelo de regressão. Por se tratar de uma porcentagem, seu valor varia de 0 a 1.

Desse modo fiz uma otimização para melhorar desempenho do modelo utilizando a técnica de otimização de hiperparâmetros, utilzando validação cruzada. Por meio da CrossValidator da biblioteca pyspark.ml.tuning e para definir o espaço de busca dos hiperparâmetros utilizei a classe ParamGridBuilder. Para finalizar o Notebook criei um outro modelo de Machine Learning implementando o algoritmo Gradient-boosted tree regression e avalisando ele.
